# ğŸ“š Module 5 â€” Introduction to RAG (Retrieval-Augmented Generation)

## ğŸ“Œ Overview

This module demonstrates a **simple Retrieval-Augmented Generation (RAG) pipeline** using:

- ğŸ” Keyword-based document retrieval
- âš¡ LiteLLM for LLM abstraction
- ğŸ”¥ Groq API for generation

Instead of directly asking the LLM, we first retrieve relevant knowledge from local documents and then use that context to generate a more accurate answer.

This is the foundation of modern AI systems like ChatGPT with knowledge bases.

---

## ğŸ¯ Objective

Build a basic RAG pipeline:

```
User Question 
      â†“
Document Retrieval (Keyword-Based)
      â†“
Context Injection
      â†“
LLM Generation
      â†“
Final Answer
```

The goal is to understand how LLMs can be grounded in external knowledge.

---

## ğŸ§  Concepts Covered

- Retrieval-Augmented Generation (RAG)
- Context injection
- Keyword-based retrieval logic
- LLM abstraction using LiteLLM
- Modular AI pipeline design
- Local knowledge base usage
- Production-style separation of components

---

## ğŸ“‚ Project Structure

```
Module5_RAG/
â”‚
â”œâ”€â”€ knowledge_base/        # Text files used as retrieval source
â”‚
â”œâ”€â”€ retriever.py           # Loads documents & performs keyword retrieval
â”œâ”€â”€ llm_layer.py           # Handles LiteLLM model interaction
â”œâ”€â”€ rag_pipeline.py        # Connects retrieval + generation
â”œâ”€â”€ main.py                # Interactive CLI for user questions
â”‚
â”œâ”€â”€ requirements.txt       # Dependencies
â””â”€â”€ .env.example           # Example environment configuration
```

---

## ğŸ— How the RAG Pipeline Works

### 1ï¸âƒ£ Knowledge Base
Contains `.txt` files with domain-specific information.

---

### 2ï¸âƒ£ Retriever (`retriever.py`)
- Loads all documents
- Performs simple keyword matching
- Selects relevant text chunks

---

### 3ï¸âƒ£ LLM Layer (`llm_layer.py`)
- Sends context + question to Groq via LiteLLM
- Handles model configuration
- Returns generated response

---

### 4ï¸âƒ£ RAG Pipeline (`rag_pipeline.py`)
- Accepts user query
- Retrieves relevant context
- Injects context into prompt
- Calls LLM
- Returns final answer

---

### 5ï¸âƒ£ Main CLI (`main.py`)
- Interactive interface
- Takes user input
- Displays final AI response

---

## âš™ Setup Instructions

### Install Dependencies

```bash
pip install -r requirements.txt
```

---

### Run the Application

```bash
python main.py
```

You can now ask questions, and the system will:

1. Search the knowledge base  
2. Retrieve relevant content  
3. Inject context into the prompt  
4. Generate grounded answers  

---

## ğŸ§¾ Example Flow

If your knowledge base contains AI-related documents:

**User Question:**  
> What is prompt engineering?

**System Process:**
- Finds documents mentioning "prompt engineering"
- Extracts relevant paragraphs
- Sends them to the LLM with the question
- Generates contextual answer

---

## ğŸ“ Learning Outcomes

After completing this module, you will:

âœ” Understand how RAG works  
âœ” Build a basic retrieval system  
âœ” Combine retrieval + generation  
âœ” Ground LLM responses in external knowledge  
âœ” Design modular GenAI architectures  

---

## ğŸš€ Why RAG Matters

Large Language Models:

- Do NOT have real-time knowledge  
- Can hallucinate  
- Cannot access private documents by default  

RAG solves this by:

- Injecting trusted context  
- Reducing hallucinations  
- Enabling domain-specific AI systems  
- Powering enterprise AI assistants  

This is the core architecture behind:

- Chatbots with knowledge bases  
- AI customer support systems  
- Internal enterprise AI tools  

---

## ğŸ”® Possible Improvements

- Replace keyword retrieval with embeddings
- Add vector database (FAISS / Chroma)
- Add chunking strategy
- Add similarity scoring
- Convert to FastAPI service
- Add web interface
- Add streaming responses
- Add caching layer

---

## ğŸ‘¨â€ğŸ’» Author

Chethan Malli  
AI & ML Enthusiast  
Building modular AI systems ğŸš€  

---

â­ If this module helped you understand RAG, give the repo a star!
